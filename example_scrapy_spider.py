# example_scrapy_spider.py
"""
VÃ­ dá»¥ Scrapy Spider Ä‘á»ƒ crawl career pages
ÄÃ¢y lÃ  cÃ¡ch Scrapy hoáº¡t Ä‘á»™ng so vá»›i code hiá»‡n táº¡i
"""

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from urllib.parse import urljoin
import json

class CareerPageSpider(scrapy.Spider):
    """
    Scrapy Spider Ä‘á»ƒ tÃ¬m career pages
    """
    name = 'career_spider'
    
    # Cáº¥u hÃ¬nh spider
    custom_settings = {
        'CONCURRENT_REQUESTS': 5,  # Crawl 5 trang cÃ¹ng lÃºc
        'DOWNLOAD_DELAY': 1,       # Delay 1 giÃ¢y giá»¯a cÃ¡c request
        'ROBOTSTXT_OBEY': True,    # TuÃ¢n thá»§ robots.txt
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    def __init__(self, start_url=None, *args, **kwargs):
        super(CareerPageSpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url] if start_url else ['https://example.com']
        self.career_pages = []
        self.crawled_pages = 0
        self.max_pages = 20
        
    def start_requests(self):
        """
        Báº¯t Ä‘áº§u crawl tá»« URL gá»‘c
        """
        for url in self.start_urls:
            # Æ¯u tiÃªn cao nháº¥t cho trang gá»‘c
            yield scrapy.Request(
                url=url,
                callback=self.parse,
                priority=10,  # Priority cao nháº¥t
                meta={'depth': 0}
            )
    
    def parse(self, response):
        """
        Parse trang gá»‘c vÃ  tÃ¬m career pages
        """
        self.crawled_pages += 1
        
        # Dá»«ng náº¿u Ä‘Ã£ crawl Ä‘á»§ pages
        if self.crawled_pages >= self.max_pages:
            return
        
        # TÃ¬m táº¥t cáº£ links trÃªn trang
        all_links = response.css('a::attr(href)').getall()
        
        # PhÃ¢n loáº¡i links theo priority
        priority_links = self.categorize_links(all_links, response.url)
        
        # Crawl theo priority
        for priority, links in priority_links.items():
            for link in links[:5]:  # Chá»‰ crawl 5 links má»—i priority
                full_url = urljoin(response.url, link)
                
                yield scrapy.Request(
                    url=full_url,
                    callback=self.parse_page,
                    priority=priority,
                    meta={'depth': response.meta.get('depth', 0) + 1}
                )
    
    def categorize_links(self, links, base_url):
        """
        PhÃ¢n loáº¡i links theo priority
        """
        career_keywords = [
            'career', 'careers', 'job', 'jobs', 'recruitment', 'employment',
            'tuyen-dung', 'viec-lam', 'co-hoi', 'nhan-vien'
        ]
        
        navigation_keywords = [
            'about', 'contact', 'company', 'team', 'services', 'products'
        ]
        
        priority_links = {
            10: [],  # Career pages (cao nháº¥t)
            8: [],   # Navigation pages
            5: [],   # Content pages
            1: []    # Other pages (tháº¥p nháº¥t)
        }
        
        for link in links:
            link_lower = link.lower()
            
            # Career pages - priority cao nháº¥t
            if any(keyword in link_lower for keyword in career_keywords):
                priority_links[10].append(link)
            
            # Navigation pages
            elif any(keyword in link_lower for keyword in navigation_keywords):
                priority_links[8].append(link)
            
            # Content pages (news, blog, etc.)
            elif any(keyword in link_lower for keyword in ['news', 'blog', 'article']):
                priority_links[5].append(link)
            
            # Other pages
            else:
                priority_links[1].append(link)
        
        return priority_links
    
    def parse_page(self, response):
        """
        Parse tá»«ng trang vÃ  kiá»ƒm tra cÃ³ pháº£i career page khÃ´ng
        """
        self.crawled_pages += 1
        
        # Dá»«ng náº¿u Ä‘Ã£ crawl Ä‘á»§ pages
        if self.crawled_pages >= self.max_pages:
            return
        
        # Kiá»ƒm tra cÃ³ pháº£i career page khÃ´ng
        if self.is_career_page(response):
            self.career_pages.append({
                'url': response.url,
                'title': response.css('title::text').get(),
                'confidence': self.calculate_confidence(response)
            })
            
            # Log káº¿t quáº£
            self.logger.info(f"ğŸ¯ Career page found: {response.url}")
        
        # TÃ¬m thÃªm links náº¿u chÆ°a Ä‘á»§ pages
        if self.crawled_pages < self.max_pages:
            links = response.css('a::attr(href)').getall()
            for link in links[:3]:  # Chá»‰ crawl 3 links má»—i trang
                full_url = urljoin(response.url, link)
                yield scrapy.Request(
                    url=full_url,
                    callback=self.parse_page,
                    priority=5,
                    meta={'depth': response.meta.get('depth', 0) + 1}
                )
    
    def is_career_page(self, response):
        """
        Kiá»ƒm tra cÃ³ pháº£i career page khÃ´ng
        """
        url = response.url.lower()
        content = response.text.lower()
        
        career_indicators = [
            'career', 'job', 'recruitment', 'employment',
            'tuyen-dung', 'viec-lam', 'co-hoi', 'nhan-vien',
            'apply', 'application', 'hiring', 'join-us'
        ]
        
        # Kiá»ƒm tra URL
        if any(indicator in url for indicator in career_indicators):
            return True
        
        # Kiá»ƒm tra content
        if any(indicator in content for indicator in career_indicators):
            return True
        
        return False
    
    def calculate_confidence(self, response):
        """
        TÃ­nh Ä‘á»™ tin cáº­y cá»§a career page
        """
        confidence = 0.0
        url = response.url.lower()
        content = response.text.lower()
        
        # URL indicators
        if 'career' in url:
            confidence += 0.4
        if 'job' in url:
            confidence += 0.3
        if 'tuyen-dung' in url:
            confidence += 0.4
        
        # Content indicators
        if 'apply' in content:
            confidence += 0.2
        if 'position' in content:
            confidence += 0.2
        if 'hiring' in content:
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def closed(self, reason):
        """
        Khi spider káº¿t thÃºc
        """
        result = {
            'success': True,
            'career_pages': self.career_pages,
            'total_pages_crawled': self.crawled_pages,
            'career_pages_found': len(self.career_pages)
        }
        
        # LÆ°u káº¿t quáº£
        with open('scrapy_result.json', 'w') as f:
            json.dump(result, f, indent=2)
        
        self.logger.info(f"âœ… Crawling completed: {self.crawled_pages} pages, {len(self.career_pages)} career pages found")

# HÃ m cháº¡y spider
def run_scrapy_spider(url):
    """
    Cháº¡y Scrapy spider
    """
    # Cáº¥u hÃ¬nh settings
    settings = get_project_settings()
    settings.update({
        'LOG_LEVEL': 'INFO',
        'FEEDS': {
            'scrapy_result.json': {
                'format': 'json',
                'encoding': 'utf8',
                'indent': 2,
            }
        }
    })
    
    # Táº¡o process
    process = CrawlerProcess(settings)
    
    # Cháº¡y spider
    process.crawl(CareerPageSpider, start_url=url)
    process.start()

# VÃ­ dá»¥ sá»­ dá»¥ng
if __name__ == "__main__":
    # Cháº¡y spider vá»›i URL
    run_scrapy_spider("https://www.ics-p.vn/vi") 