# app/services/scrapy_career_spider.py
"""
Optimized Scrapy Spider for career page detection
T·ªëi ∆∞u keywords v√† performance
"""

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from urllib.parse import urljoin, urlparse
import json
import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

class OptimizedCareerSpider(scrapy.Spider):
    """
    Optimized Scrapy Spider v·ªõi keywords t·ªëi ∆∞u
    """
    name = 'optimized_career_spider'
    
    # C·∫•u h√¨nh t·ªëi ∆∞u
    custom_settings = {
        'CONCURRENT_REQUESTS': 8,        # TƒÉng l√™n 8 trang c√πng l√∫c
        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,
        'DOWNLOAD_DELAY': 0.5,           # Gi·∫£m delay xu·ªëng 0.5s
        'ROBOTSTXT_OBEY': False,         # T·∫Øt robots.txt ƒë·ªÉ crawl nhanh h∆°n
        'COOKIES_ENABLED': False,        # T·∫Øt cookies ƒë·ªÉ tƒÉng t·ªëc
        'DOWNLOAD_TIMEOUT': 15,          # Timeout 15s
        'RETRY_TIMES': 2,                # Retry 2 l·∫ßn
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    def __init__(self, start_url: str = None, max_pages: int = 20, *args, **kwargs):
        super(OptimizedCareerSpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url] if start_url else ['https://example.com']
        self.max_pages = max_pages
        self.career_pages = []
        self.crawled_pages = 0
        self.found_career_pages = 0
        self.domain = None
        
    def start_requests(self):
        """
        B·∫Øt ƒë·∫ßu crawl v·ªõi priority cao nh·∫•t
        """
        for url in self.start_urls:
            self.domain = urlparse(url).netloc
            logger.info(f"üöÄ Starting optimized career spider for: {url}")
            
            yield scrapy.Request(
                url=url,
                callback=self.parse_homepage,
                priority=100,  # Priority cao nh·∫•t
                meta={'depth': 0, 'is_homepage': True}
            )
    
    def parse_homepage(self, response):
        """
        Parse homepage v·ªõi focus v√†o navigation
        """
        self.crawled_pages += 1
        logger.info(f"üìÑ Crawling homepage: {response.url}")
        
        # T√¨m t·∫•t c·∫£ links tr√™n homepage
        all_links = self.extract_all_links(response)
        
        # Ph√¢n lo·∫°i v√† ∆∞u ti√™n links
        prioritized_links = self.prioritize_links(all_links, response.url)
        
        # Crawl theo priority
        for priority, links in prioritized_links.items():
            for link in links:
                if self.crawled_pages >= self.max_pages:
                    break
                    
                full_url = urljoin(response.url, link)
                
                # Ch·ªâ crawl c√πng domain
                if urlparse(full_url).netloc == self.domain:
                    yield scrapy.Request(
                        url=full_url,
                        callback=self.parse_page,
                        priority=priority,
                        meta={'depth': 1, 'priority': priority}
                    )
    
    def extract_all_links(self, response) -> List[str]:
        """
        Extract t·∫•t c·∫£ links v·ªõi focus v√†o navigation
        """
        links = []
        
        # ∆Øu ti√™n navigation links
        nav_selectors = [
            'nav a::attr(href)',
            'header a::attr(href)', 
            '.navbar a::attr(href)',
            '.menu a::attr(href)',
            '.navigation a::attr(href)',
            '.main-menu a::attr(href)',
            '.top-menu a::attr(href)'
        ]
        
        for selector in nav_selectors:
            nav_links = response.css(selector).getall()
            links.extend(nav_links)
        
        # Th√™m footer links
        footer_links = response.css('footer a::attr(href)').getall()
        links.extend(footer_links)
        
        # Th√™m t·∫•t c·∫£ links kh√°c
        all_links = response.css('a::attr(href)').getall()
        links.extend(all_links)
        
        # Remove duplicates v√† filter
        unique_links = list(set(links))
        filtered_links = [link for link in unique_links if self.is_valid_link(link)]
        
        return filtered_links
    
    def is_valid_link(self, link: str) -> bool:
        """
        Ki·ªÉm tra link c√≥ h·ª£p l·ªá kh√¥ng
        """
        if not link or link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
            return False
        
        # Lo·∫°i b·ªè external links
        if link.startswith('http') and self.domain not in link:
            return False
            
        return True
    
    def prioritize_links(self, links: List[str], base_url: str) -> Dict[int, List[str]]:
        """
        Ph√¢n lo·∫°i links theo priority v·ªõi keywords t·ªëi ∆∞u
        """
        # Keywords t·ªëi ∆∞u cho career pages
        career_keywords = [
            # Vietnamese keywords (t·ªëi ∆∞u)
            'tuyen-dung', 'tuy·ªÉn-d·ª•ng', 'tuyendung',
            'viec-lam', 'vi·ªác-l√†m', 'vieclam', 
            'co-hoi', 'c∆°-h·ªôi', 'cohoi',
            'nhan-vien', 'nh√¢n-vi√™n', 'nhanvien',
            'ung-vien', '·ª©ng-vi√™n', 'ungvien',
            'cong-viec', 'c√¥ng-vi·ªác', 'congviec',
            'lam-viec', 'l√†m-vi·ªác', 'lamviec',
            'thu-viec', 'th·ª≠-vi·ªác', 'thuviec',
            'chinh-thuc', 'ch√≠nh-th·ª©c', 'chinhthuc',
            'nghe-nghiep', 'ngh·ªÅ-nghi·ªáp', 'nghenghiep',
            'tim-viec', 't√¨m-vi·ªác', 'timviec',
            'dang-tuyen', 'ƒëang-tuy·ªÉn', 'dangtuyen',
            
            # English keywords (t·ªëi ∆∞u)
            'career', 'careers', 'job', 'jobs',
            'recruitment', 'employment', 'hiring',
            'work', 'position', 'opportunity', 'vacancy',
            'apply', 'application', 'join-us',
            'team', 'talent', 'open-role', 'open-roles',
            'we-are-hiring', 'work-with-us', 'join-our-team',
            'grow-with-us', 'build-with-us', 'create-with-us',
            'full-time', 'part-time', 'remote', 'hybrid',
            'onsite', 'on-site', 'freelance', 'contract',
            'internship', 'intern', 'graduate', 'entry-level',
            'senior', 'junior', 'lead', 'principal'
        ]
        
        # Keywords cho navigation pages
        nav_keywords = [
            'about', 'about-us', 'company', 'team', 'contact',
            'services', 'products', 'solutions', 'portfolio',
            'gi·ªõi-thi·ªáu', 'c√¥ng-ty', 'ƒë·ªôi-ng≈©', 'li√™n-h·ªá',
            'd·ªãch-v·ª•', 's·∫£n-ph·∫©m', 'gi·∫£i-ph√°p'
        ]
        
        # Keywords cho content pages
        content_keywords = [
            'news', 'blog', 'article', 'press', 'media',
            'tin-t·ª©c', 'b√†i-vi·∫øt', 'th√¥ng-c√°o', 'truy·ªÅn-th√¥ng'
        ]
        
        priority_links = {
            100: [],  # Career pages (cao nh·∫•t)
            80: [],   # Navigation pages
            50: [],   # Content pages  
            10: []    # Other pages (th·∫•p nh·∫•t)
        }
        
        for link in links:
            link_lower = link.lower()
            
            # Career pages - priority cao nh·∫•t
            if any(keyword in link_lower for keyword in career_keywords):
                priority_links[100].append(link)
                logger.info(f"üéØ Career link found: {link}")
            
            # Navigation pages
            elif any(keyword in link_lower for keyword in nav_keywords):
                priority_links[80].append(link)
            
            # Content pages
            elif any(keyword in link_lower for keyword in content_keywords):
                priority_links[50].append(link)
            
            # Other pages
            else:
                priority_links[10].append(link)
        
        return priority_links
    
    def parse_page(self, response):
        """
        Parse t·ª´ng trang v·ªõi detection t·ªëi ∆∞u
        """
        self.crawled_pages += 1
        priority = response.meta.get('priority', 10)
        
        logger.info(f"üìÑ Crawling page {self.crawled_pages}/{self.max_pages}: {response.url} (priority: {priority})")
        
        # Ki·ªÉm tra c√≥ ph·∫£i career page kh√¥ng
        career_score = self.calculate_career_score(response)
        
        if career_score >= 0.6:  # Threshold cao h∆°n ƒë·ªÉ ch√≠nh x√°c
            career_page = {
                'url': response.url,
                'title': response.css('title::text').get() or '',
                'confidence': career_score,
                'indicators': self.get_career_indicators(response),
                'priority_found': priority
            }
            
            self.career_pages.append(career_page)
            self.found_career_pages += 1
            
            logger.info(f"üéØ Career page found: {response.url} (score: {career_score:.2f})")
        
        # D·ª´ng n·∫øu ƒë√£ crawl ƒë·ªß pages ho·∫∑c t√¨m th·∫•y ƒë·ªß career pages
        if self.crawled_pages >= self.max_pages or self.found_career_pages >= 3:
            return
        
        # T√¨m th√™m links n·∫øu c·∫ßn
        if self.crawled_pages < self.max_pages:
            links = self.extract_all_links(response)
            prioritized_links = self.prioritize_links(links, response.url)
            
            for priority, link_list in prioritized_links.items():
                for link in link_list[:2]:  # Ch·ªâ crawl 2 links m·ªói priority
                    if self.crawled_pages >= self.max_pages:
                        break
                        
                    full_url = urljoin(response.url, link)
                    if urlparse(full_url).netloc == self.domain:
                        yield scrapy.Request(
                            url=full_url,
                            callback=self.parse_page,
                            priority=priority,
                            meta={'depth': response.meta.get('depth', 0) + 1, 'priority': priority}
                        )
    
    def calculate_career_score(self, response) -> float:
        """
        T√≠nh ƒëi·ªÉm career page v·ªõi algorithm t·ªëi ∆∞u
        """
        url = response.url.lower()
        content = response.text.lower()
        title = response.css('title::text').get('').lower()
        
        score = 0.0
        
        # URL indicators (weight: 0.4)
        url_indicators = [
            'career', 'careers', 'job', 'jobs', 'recruitment', 'employment',
            'tuyen-dung', 'viec-lam', 'co-hoi', 'nhan-vien', 'ung-vien',
            'cong-viec', 'lam-viec', 'thu-viec', 'chinh-thuc', 'nghe-nghiep'
        ]
        
        for indicator in url_indicators:
            if indicator in url:
                score += 0.4
                break
        
        # Title indicators (weight: 0.3)
        title_indicators = [
            'career', 'job', 'recruitment', 'employment', 'hiring',
            'tuy·ªÉn d·ª•ng', 'vi·ªác l√†m', 'c∆° h·ªôi', 'nh√¢n vi√™n', '·ª©ng vi√™n',
            'c√¥ng vi·ªác', 'l√†m vi·ªác', 'th·ª≠ vi·ªác', 'ch√≠nh th·ª©c', 'ngh·ªÅ nghi·ªáp'
        ]
        
        for indicator in title_indicators:
            if indicator in title:
                score += 0.3
                break
        
        # Content indicators (weight: 0.3)
        content_indicators = [
            'apply', 'application', 'submit', 'join', 'work with us',
            'position', 'role', 'opportunity', 'vacancy', 'opening',
            'hiring', 'recruiting', 'employment', 'career opportunity',
            '·ª©ng tuy·ªÉn', 'n·ªôp ƒë∆°n', 'tham gia', 'l√†m vi·ªác c√πng ch√∫ng t√¥i',
            'v·ªã tr√≠', 'c∆° h·ªôi', 'tuy·ªÉn d·ª•ng', 'vi·ªác l√†m'
        ]
        
        for indicator in content_indicators:
            if indicator in content:
                score += 0.1
                if score >= 0.6:  # ƒê·ªß ƒëi·ªÉm r·ªìi
                    break
        
        return min(score, 1.0)
    
    def get_career_indicators(self, response) -> List[str]:
        """
        L·∫•y danh s√°ch indicators t√¨m th·∫•y
        """
        indicators = []
        url = response.url.lower()
        content = response.text.lower()
        title = response.css('title::text').get('').lower()
        
        # URL indicators
        if 'career' in url:
            indicators.append('URL contains career')
        if 'job' in url:
            indicators.append('URL contains job')
        if 'tuyen-dung' in url:
            indicators.append('URL contains tuyen-dung')
        
        # Title indicators
        if 'career' in title:
            indicators.append('Title contains career')
        if 'job' in title:
            indicators.append('Title contains job')
        
        # Content indicators
        if 'apply' in content:
            indicators.append('Content contains apply')
        if 'position' in content:
            indicators.append('Content contains position')
        if 'hiring' in content:
            indicators.append('Content contains hiring')
        
        return indicators
    
    def closed(self, reason):
        """
        Khi spider k·∫øt th√∫c
        """
        # Convert career_pages to list of URLs
        career_page_urls = []
        for page in self.career_pages:
            if isinstance(page, dict):
                career_page_urls.append(page.get('url', ''))
            else:
                career_page_urls.append(str(page))
        
        result = {
            'success': True,
            'requested_url': self.start_urls[0] if self.start_urls else '',
            'career_pages': career_page_urls,
            'total_pages_crawled': self.crawled_pages,
            'career_pages_found': len(self.career_pages),
            'crawl_time': getattr(self, 'crawl_time', 0),
            'crawl_method': 'scrapy_optimized'
        }
        
        logger.info(f"‚úÖ Optimized crawling completed: {self.crawled_pages} pages, {len(self.career_pages)} career pages found")
        
        return result

# H√†m ch·∫°y spider
async def run_optimized_career_spider(url: str, max_pages: int = 20) -> Dict:
    """
    Ch·∫°y optimized Scrapy spider
    """
    try:
        # C·∫•u h√¨nh settings
        settings = get_project_settings()
        settings.update({
            'LOG_LEVEL': 'INFO',
            'LOG_FORMAT': '%(asctime)s [%(name)s] %(levelname)s: %(message)s',
            'FEEDS': {
                'scrapy_result.json': {
                    'format': 'json',
                    'encoding': 'utf8',
                    'indent': 2,
                }
            }
        })
        
        # T·∫°o process
        process = CrawlerProcess(settings)
        
        # Ch·∫°y spider
        process.crawl(OptimizedCareerSpider, start_url=url, max_pages=max_pages)
        process.start()
        
        # ƒê·ªçc k·∫øt qu·∫£
        try:
            with open('scrapy_result.json', 'r') as f:
                result = json.load(f)
            return result
        except FileNotFoundError:
            return {
                'success': False,
                'error_message': 'No result file found'
            }
            
    except Exception as e:
        logger.error(f"Error running optimized spider: {e}")
        return {
            'success': False,
            'error_message': str(e)
        } 