# app/services/scrapy_career_spider.py
"""
Optimized Scrapy Spider for career page detection
T·ªëi ∆∞u keywords v√† performance
"""

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from urllib.parse import urljoin, urlparse
import json
import logging
import time
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

class OptimizedCareerSpider(scrapy.Spider):
    """
    Optimized Scrapy Spider v·ªõi keywords t·ªëi ∆∞u
    """
    name = 'optimized_career_spider'
    
    # C·∫•u h√¨nh t·ªëi ∆∞u cho speed
    custom_settings = {
        'CONCURRENT_REQUESTS': 16,       # TƒÉng l√™n 16 trang c√πng l√∫c
        'CONCURRENT_REQUESTS_PER_DOMAIN': 16,
        'DOWNLOAD_DELAY': 0.1,           # Gi·∫£m delay xu·ªëng 0.1s
        'ROBOTSTXT_OBEY': False,         # T·∫Øt robots.txt ƒë·ªÉ crawl nhanh h∆°n
        'COOKIES_ENABLED': False,        # T·∫Øt cookies ƒë·ªÉ tƒÉng t·ªëc
        'DOWNLOAD_TIMEOUT': 10,          # Timeout 10s
        'RETRY_TIMES': 1,                # Retry 1 l·∫ßn
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        # T·∫Øt extensions kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ log s·∫°ch h∆°n
        'TELNETCONSOLE_ENABLED': False,  # T·∫Øt Telnet ƒë·ªÉ tr√°nh l·ªói ConnectionDone
        'LOGSTATS_INTERVAL': 60,         # Gi·∫£m log stats frequency
        'MEMUSAGE_ENABLED': False,       # T·∫Øt memory usage tracking
        'AUTOTHROTTLE_ENABLED': False,   # T·∫Øt auto throttle
        'AUTOTHROTTLE_START_DELAY': 0,   # Kh√¥ng delay
        'AUTOTHROTTLE_MAX_DELAY': 0      # Kh√¥ng delay
    }
    
    def __init__(self, start_url: str = None, max_pages: int = 50, *args, **kwargs):
        super(OptimizedCareerSpider, self).__init__(*args, **kwargs)
        self.start_urls = [start_url] if start_url else ['https://example.com']
        self.max_pages = max_pages
        self.career_pages = []
        self.crawled_pages = 0
        self.found_career_pages = 0
        self.domain = None
        self.start_time = time.time()
        
    def start_requests(self):
        """
        B·∫Øt ƒë·∫ßu crawl v·ªõi priority cao nh·∫•t
        """
        for url in self.start_urls:
            self.domain = urlparse(url).netloc
            logger.info(f"üöÄ Starting optimized career spider for: {url}")
            
            yield scrapy.Request(
                url=url,
                callback=self.parse_homepage,
                priority=100,  # Priority cao nh·∫•t
                meta={'depth': 0, 'is_homepage': True}
            )
    
    def parse_homepage(self, response):
        """
        Parse homepage v·ªõi focus v√†o navigation
        """
        self.crawled_pages += 1
        logger.info(f"üìÑ Crawling homepage: {response.url}")
        
        # T√¨m t·∫•t c·∫£ links tr√™n homepage
        all_links = self.extract_all_links(response)
        
        # Ph√¢n lo·∫°i v√† ∆∞u ti√™n links
        prioritized_links = self.prioritize_links(all_links, response.url)
        
        # Crawl theo priority
        for priority, links in prioritized_links.items():
            for link in links:
                if self.crawled_pages >= self.max_pages:
                    break
                    
                full_url = urljoin(response.url, link)
                
                # Ch·ªâ crawl c√πng domain
                if urlparse(full_url).netloc == self.domain:
                    yield scrapy.Request(
                        url=full_url,
                        callback=self.parse_page,
                        priority=priority,
                        meta={'depth': 1, 'priority': priority}
                    )
    
    def extract_all_links(self, response) -> List[str]:
        """
        Extract t·∫•t c·∫£ links v·ªõi focus v√†o navigation
        """
        links = []
        
        # ∆Øu ti√™n navigation links
        nav_selectors = [
            'nav a::attr(href)',
            'header a::attr(href)', 
            '.navbar a::attr(href)',
            '.menu a::attr(href)',
            '.navigation a::attr(href)',
            '.main-menu a::attr(href)',
            '.top-menu a::attr(href)'
        ]
        
        for selector in nav_selectors:
            nav_links = response.css(selector).getall()
            links.extend(nav_links)
        
        # Th√™m footer links
        footer_links = response.css('footer a::attr(href)').getall()
        links.extend(footer_links)
        
        # Th√™m t·∫•t c·∫£ links kh√°c
        all_links = response.css('a::attr(href)').getall()
        links.extend(all_links)
        
        # Remove duplicates v√† filter
        unique_links = list(set(links))
        filtered_links = [link for link in unique_links if self.is_valid_link(link)]
        
        return filtered_links
    
    def is_valid_link(self, link: str) -> bool:
        """
        Ki·ªÉm tra link c√≥ h·ª£p l·ªá kh√¥ng
        """
        if not link or link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
            return False
        
        # Lo·∫°i b·ªè external links
        if link.startswith('http') and self.domain not in link:
            return False
            
        return True
    
    def prioritize_links(self, links: List[str], base_url: str) -> Dict[int, List[str]]:
        """
        Ph√¢n lo·∫°i links theo priority v·ªõi keywords t·ªëi ∆∞u
        """
        # Keywords t·ªëi ∆∞u cho career pages
        career_keywords = [
            # Vietnamese keywords (t·ªëi ∆∞u)
            'tuyen-dung', 'tuy·ªÉn-d·ª•ng', 'tuyendung',
            'viec-lam', 'vi·ªác-l√†m', 'vieclam', 
            'co-hoi', 'c∆°-h·ªôi', 'cohoi',
            'nhan-vien', 'nh√¢n-vi√™n', 'nhanvien',
            'ung-vien', '·ª©ng-vi√™n', 'ungvien',
            'cong-viec', 'c√¥ng-vi·ªác', 'congviec',
            'lam-viec', 'l√†m-vi·ªác', 'lamviec',
            'thu-viec', 'th·ª≠-vi·ªác', 'thuviec',
            'chinh-thuc', 'ch√≠nh-th·ª©c', 'chinhthuc',
            'nghe-nghiep', 'ngh·ªÅ-nghi·ªáp', 'nghenghiep',
            'tim-viec', 't√¨m-vi·ªác', 'timviec',
            'dang-tuyen', 'ƒëang-tuy·ªÉn', 'dangtuyen',
            
            # English keywords (t·ªëi ∆∞u)
            'career', 'careers', 'job', 'jobs',
            'recruitment', 'employment', 'hiring',
            'work', 'position', 'opportunity', 'vacancy',
            'apply', 'application', 'join-us',
            'team', 'talent', 'open-role', 'open-roles',
            'we-are-hiring', 'work-with-us', 'join-our-team',
            'grow-with-us', 'build-with-us', 'create-with-us',
            'full-time', 'part-time', 'remote', 'hybrid',
            'onsite', 'on-site', 'freelance', 'contract',
            'internship', 'intern', 'graduate', 'entry-level',
            'senior', 'junior', 'lead', 'principal'
        ]
        
        # Keywords cho navigation pages
        nav_keywords = [
            'about', 'about-us', 'company', 'team', 'contact',
            'services', 'products', 'solutions', 'portfolio',
            'gi·ªõi-thi·ªáu', 'c√¥ng-ty', 'ƒë·ªôi-ng≈©', 'li√™n-h·ªá',
            'd·ªãch-v·ª•', 's·∫£n-ph·∫©m', 'gi·∫£i-ph√°p'
        ]
        
        # Keywords cho content pages
        content_keywords = [
            'news', 'blog', 'article', 'press', 'media',
            'tin-t·ª©c', 'b√†i-vi·∫øt', 'th√¥ng-c√°o', 'truy·ªÅn-th√¥ng'
        ]
        
        priority_links = {
            100: [],  # Career pages (cao nh·∫•t)
            80: [],   # Navigation pages
            50: [],   # Content pages  
            10: []    # Other pages (th·∫•p nh·∫•t)
        }
        
        for link in links:
            link_lower = link.lower()
            
            # Career pages - priority cao nh·∫•t
            if any(keyword in link_lower for keyword in career_keywords):
                priority_links[100].append(link)
                logger.info(f"üéØ Career link found: {link}")
            
            # Navigation pages
            elif any(keyword in link_lower for keyword in nav_keywords):
                priority_links[80].append(link)
            
            # Content pages
            elif any(keyword in link_lower for keyword in content_keywords):
                priority_links[50].append(link)
            
            # Other pages
            else:
                priority_links[10].append(link)
        
        return priority_links
    
    def parse_page(self, response):
        """
        Parse t·ª´ng trang v·ªõi detection t·ªëi ∆∞u
        """
        self.crawled_pages += 1
        priority = response.meta.get('priority', 10)
        
        logger.info(f"üìÑ Crawling page {self.crawled_pages}/{self.max_pages}: {response.url} (priority: {priority})")
        
        # Ki·ªÉm tra c√≥ ph·∫£i career page kh√¥ng
        career_score = self.calculate_career_score(response)
        
        if career_score >= 0.6:  # Threshold cao h∆°n ƒë·ªÉ ch√≠nh x√°c
            career_page = {
                'url': response.url,
                'title': response.css('title::text').get() or '',
                'confidence': career_score,
                'indicators': self.get_career_indicators(response),
                'priority_found': priority
            }
            
            self.career_pages.append(career_page)
            self.found_career_pages += 1
            
            logger.info(f"üéØ Career page found: {response.url} (score: {career_score:.2f})")
        
        # D·ª´ng n·∫øu ƒë√£ crawl ƒë·ªß pages ho·∫∑c t√¨m th·∫•y ƒë·ªß career pages
        if self.crawled_pages >= self.max_pages or self.found_career_pages >= 3:
            return
        
        # T√¨m th√™m links n·∫øu c·∫ßn
        if self.crawled_pages < self.max_pages:
            links = self.extract_all_links(response)
            prioritized_links = self.prioritize_links(links, response.url)
            
            for priority, link_list in prioritized_links.items():
                for link in link_list[:2]:  # Ch·ªâ crawl 2 links m·ªói priority
                    if self.crawled_pages >= self.max_pages:
                        break
                        
                    full_url = urljoin(response.url, link)
                    if urlparse(full_url).netloc == self.domain:
                        yield scrapy.Request(
                            url=full_url,
                            callback=self.parse_page,
                            priority=priority,
                            meta={'depth': response.meta.get('depth', 0) + 1, 'priority': priority}
                        )
    
    def calculate_career_score(self, response) -> float:
        """
        T√≠nh ƒëi·ªÉm career page v·ªõi algorithm t·ªëi ∆∞u
        """
        url = response.url.lower()
        content = response.text.lower()
        title = response.css('title::text').get('').lower()
        
        score = 0.0
        
        # URL indicators (weight: 0.4)
        url_indicators = [
            'career', 'careers', 'job', 'jobs', 'recruitment', 'employment',
            'tuyen-dung', 'viec-lam', 'co-hoi', 'nhan-vien', 'ung-vien',
            'cong-viec', 'lam-viec', 'thu-viec', 'chinh-thuc', 'nghe-nghiep'
        ]
        
        for indicator in url_indicators:
            if indicator in url:
                score += 0.4
                break
        
        # Title indicators (weight: 0.3)
        title_indicators = [
            'career', 'job', 'recruitment', 'employment', 'hiring',
            'tuy·ªÉn d·ª•ng', 'vi·ªác l√†m', 'c∆° h·ªôi', 'nh√¢n vi√™n', '·ª©ng vi√™n',
            'c√¥ng vi·ªác', 'l√†m vi·ªác', 'th·ª≠ vi·ªác', 'ch√≠nh th·ª©c', 'ngh·ªÅ nghi·ªáp'
        ]
        
        for indicator in title_indicators:
            if indicator in title:
                score += 0.3
                break
        
        # Content indicators (weight: 0.3)
        content_indicators = [
            'apply', 'application', 'submit', 'join', 'work with us',
            'position', 'role', 'opportunity', 'vacancy', 'opening',
            'hiring', 'recruiting', 'employment', 'career opportunity',
            '·ª©ng tuy·ªÉn', 'n·ªôp ƒë∆°n', 'tham gia', 'l√†m vi·ªác c√πng ch√∫ng t√¥i',
            'v·ªã tr√≠', 'c∆° h·ªôi', 'tuy·ªÉn d·ª•ng', 'vi·ªác l√†m'
        ]
        
        for indicator in content_indicators:
            if indicator in content:
                score += 0.1
                if score >= 0.6:  # ƒê·ªß ƒëi·ªÉm r·ªìi
                    break
        
        return min(score, 1.0)
    
    def get_career_indicators(self, response) -> List[str]:
        """
        L·∫•y danh s√°ch indicators t√¨m th·∫•y
        """
        indicators = []
        url = response.url.lower()
        content = response.text.lower()
        title = response.css('title::text').get('').lower()
        
        # URL indicators
        if 'career' in url:
            indicators.append('URL contains career')
        if 'job' in url:
            indicators.append('URL contains job')
        if 'tuyen-dung' in url:
            indicators.append('URL contains tuyen-dung')
        
        # Title indicators
        if 'career' in title:
            indicators.append('Title contains career')
        if 'job' in title:
            indicators.append('Title contains job')
        
        # Content indicators
        if 'apply' in content:
            indicators.append('Content contains apply')
        if 'position' in content:
            indicators.append('Content contains position')
        if 'hiring' in content:
            indicators.append('Content contains hiring')
        
        return indicators
    
    def closed(self, reason):
        """
        Khi spider k·∫øt th√∫c
        """
        # Gi·ªØ nguy√™n career_pages dict structure
        career_pages_data = []
        for page in self.career_pages:
            if isinstance(page, dict):
                career_pages_data.append(page)
            else:
                # Fallback n·∫øu page l√† string
                career_pages_data.append({
                    'url': str(page),
                    'title': '',
                    'confidence': 0.0,
                    'indicators': [],
                    'priority_found': 10
                })
        
        result = {
            'success': True,
            'requested_url': self.start_urls[0] if self.start_urls else '',
            'career_pages': career_pages_data,
            'total_pages_crawled': self.crawled_pages,
            'career_pages_found': len(self.career_pages),
            'crawl_time': getattr(self, 'crawl_time', 0),
            'crawl_method': 'scrapy_optimized'
        }
        
        logger.info(f"‚úÖ Optimized crawling completed: {self.crawled_pages} pages, {len(self.career_pages)} career pages found")
        logger.info(f"üîç Career pages data: {self.career_pages}")
        logger.info(f"üîç Final result to write: {result}")
        
        # Ghi k·∫øt qu·∫£ tr·ª±c ti·∫øp - KH√îNG d√πng FeedExporter
        import json
        import os
        import time
        
        # Ghi tr·ª±c ti·∫øp v√†o result file
        result_file = f'scrapy_result_{int(time.time())}.json'
        logger.info(f"üîç Writing to result file: {result_file}")
        try:
            # Ghi tr·ª±c ti·∫øp 1 object JSON duy nh·∫•t
            with open(result_file, 'w') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úÖ Successfully wrote result to {result_file}")
        except Exception as e:
            logger.error(f"Error writing result file: {e}")
        
        return result

# H√†m ch·∫°y spider
async def run_optimized_career_spider(url: str, max_pages: int = 100) -> Dict:
    """
    Ch·∫°y optimized Scrapy spider b·∫±ng subprocess ƒë·ªÉ tr√°nh reactor conflicts
    """
    try:
        import os
        import time
        import subprocess
        import asyncio
        
        # T·∫°o unique result file
        result_file = f'scrapy_result_{int(time.time())}.json'
        
        # T·∫°o temporary script ƒë·ªÉ ch·∫°y Scrapy
        script_content = f'''
import sys
import os
sys.path.append(os.getcwd())

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from app.services.scrapy_career_spider import OptimizedCareerSpider
import json

# C·∫•u h√¨nh settings - T·∫ÆT HO√ÄN TO√ÄN FeedExporter
settings = get_project_settings()
settings.update({{
    'LOG_LEVEL': 'INFO',
    'TELNETCONSOLE_ENABLED': False,
    'LOGSTATS_INTERVAL': 60,
    'MEMUSAGE_ENABLED': False,
    # T·∫ÆT FEEDS ƒë·ªÉ tr√°nh conflict v·ªõi manual file writing
    'FEEDS': None,
    'FEED_EXPORT_ENABLED': False,
    'FEED_EXPORT_ENCODING': 'utf-8',
    'FEED_EXPORT_INDENT': 2
}})

# Ch·∫°y spider - truy·ªÅn class, kh√¥ng ph·∫£i instance
process = CrawlerProcess(settings)
process.crawl(OptimizedCareerSpider, start_url='{url}', max_pages={max_pages})
process.start()

print("Scrapy completed successfully")
'''
        
        # L∆∞u script t·∫°m
        script_file = f'scrapy_script_{int(time.time())}.py'
        with open(script_file, 'w') as f:
            f.write(script_content)
        
        # Ch·∫°y script b·∫±ng subprocess
        start_time = time.time()
        
        process = await asyncio.create_subprocess_exec(
            'python', script_file,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        crawl_time = time.time() - start_time
        
        # Cleanup script
        try:
            os.remove(script_file)
        except:
            pass
        
        # Ki·ªÉm tra k·∫øt qu·∫£
        if process.returncode != 0:
            logger.error(f"Scrapy subprocess failed: {stderr.decode()}")
            return {
                'success': False,
                'error_message': f'Scrapy subprocess failed: {stderr.decode()}',
                'crawl_time': crawl_time,
                'crawl_method': 'scrapy_optimized'
            }
        
        # ƒê·ªçc k·∫øt qu·∫£ - t√¨m file m·ªõi nh·∫•t
        try:
            # T√¨m file scrapy_result_*.json m·ªõi nh·∫•t
            import glob
            result_files = glob.glob('scrapy_result_*.json')
            if result_files:
                # L·∫•y file m·ªõi nh·∫•t
                result_file = max(result_files, key=os.path.getctime)
                logger.info(f"üîç Found result file: {result_file}")
            else:
                raise FileNotFoundError("No result files found")
            
            with open(result_file, 'r') as f:
                content = f.read()
                logger.info(f"üîç Raw file content: {content[:200]}...")  # Debug log
                
                # Handle multiple JSON objects or extra data
                try:
                    result = json.loads(content)
                    logger.info(f"üîç Parsed JSON result type: {type(result)}")
                    logger.info(f"üîç Parsed JSON result: {result}")
                except json.JSONDecodeError:
                    # Try to find the last valid JSON object
                    lines = content.strip().split('\n')
                    logger.info(f"üîç Trying to parse {len(lines)} lines...")
                    for line in reversed(lines):
                        if line.strip():
                            try:
                                result = json.loads(line)
                                logger.info(f"üîç Found valid JSON in line: {result}")
                                break
                            except json.JSONDecodeError:
                                continue
                    else:
                        raise json.JSONDecodeError("No valid JSON found", content, 0)
            
            # Cleanup result file
            try:
                os.remove(result_file)
            except:
                pass
            return result
        except FileNotFoundError:
            return {
                'success': False,
                'error_message': 'No result file found',
                'crawl_time': crawl_time,
                'crawl_method': 'scrapy_optimized'
            }
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {e}")
            return {
                'success': False,
                'error_message': f'Invalid JSON format: {str(e)}',
                'crawl_time': crawl_time,
                'crawl_method': 'scrapy_optimized'
            }
            
    except Exception as e:
        logger.error(f"Error running optimized spider: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return {
            'success': False,
            'error_message': str(e)
        } 