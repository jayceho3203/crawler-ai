# app/services/career_pages_service.py
"""
Enhanced career page detection service
"""

import logging
import asyncio
from typing import List, Dict, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse
from datetime import datetime
import re
import os
import json
import socket

from ..utils.constants import CAREER_KEYWORDS_VI, CAREER_SELECTORS, JOB_BOARD_DOMAINS
from ..services.career_detector import filter_career_urls
from .crawler import crawl_single_url
from .scrapy_runner import run_spider
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class CareerPagesService:
    """Enhanced service for detecting career pages"""
    
    def __init__(self):
        self.career_keywords = CAREER_KEYWORDS_VI + [
            'career', 'careers', 'jobs', 'employment', 'work-with-us',
            'join-us', 'opportunities', 'vacancies', 'positions',
            'tuyen-dung', 'viec-lam', 'co-hoi', 'tuyen-nhan-vien'
        ]
        
        self.job_board_domains = list(JOB_BOARD_DOMAINS) + [
            'jobs.vn', 'careerlink.vn', 'topcv.vn', 'mywork.vn',
            'indeed.com', 'linkedin.com/jobs', 'glassdoor.com'
        ]
        
        # Performance configuration
        self.GLOBAL_TIMEOUT = 60
        self.PER_TASK_TIMEOUT = 12
        self.MAX_CONCURRENCY = 20
        self.HTTP_TIMEOUT = 8.0
        
        # Career keywords for content analysis (NOT for subdomain generation)
        self.CAREER_KEYWORDS = [
            # EN
            r"\bcareers?\b", r"\bjobs?\b", r"\bemployment\b", r"\bjoin\s+us\b",
            r"\bwork\s+with\s+us\b", r"\bopenings?\b", r"\bvacanc(?:y|ies)\b",
            # VI
            r"\btuyá»ƒn\s*dá»¥ng\b", r"\bviá»‡c\s*lÃ m\b", r"\bcÆ¡\s*há»™i\s*nghá»\s*nghiá»‡p\b",
            r"\bgia nháº­p\b", r"\bá»©ng tuyá»ƒn\b", r"\btin tuyá»ƒn\b",
        ]
        self.career_re = re.compile("|".join(self.CAREER_KEYWORDS), flags=re.IGNORECASE)
    
    def _safe_domain(self, base_url: str) -> Tuple[str, str]:
        """Extract root domain and netloc safely"""
        parsed = urlparse(base_url if '://' in base_url else f"https://{base_url}")
        netloc = parsed.netloc or parsed.path
        netloc = netloc.lower().strip().rstrip('/')
        if netloc.startswith('www.'):
            netloc = netloc[4:]
        
        # Simple root domain extraction
        parts = netloc.split('.')
        root = netloc
        if len(parts) >= 3:
            # Keep last 2-3 parts as TLD + SLD
            root = '.'.join(parts[-3:]) if len(parts[-1]) <= 2 else '.'.join(parts[-2:])
        
        return root, netloc
    
    def _is_subdomain_of(self, candidate_host: str, root_domain: str) -> bool:
        """Check if candidate is subdomain of root domain"""
        c = candidate_host.lower().strip('.')
        r = root_domain.lower().strip('.')
        return c.endswith('.' + r) and c != r
    
    def _normalize_url(self, url: str, base: str) -> str:
        """Normalize URL with base"""
        try:
            return urljoin(base, url)
        except Exception:
            return url
    
    def _collect_hosts_from_html(self, html: str, base_url: str) -> Set[str]:
        """Extract all hostnames from HTML content"""
        hosts: Set[str] = set()
        soup = BeautifulSoup(html, 'html.parser')
        
        def _push(u: str):
            if not u:
                return
            absu = self._normalize_url(u, base_url)
            p = urlparse(absu)
            if p.netloc:
                hosts.add(p.netloc.lower())
        
        # Extract from common URL attributes
        for tag, attr in [('a', 'href'), ('link', 'href'), ('script', 'src'),
                          ('img', 'src'), ('form', 'action'), ('source', 'src'),
                          ('iframe', 'src')]:
            for el in soup.find_all(tag):
                _push(el.get(attr))
        
        # Extract from inline scripts/styles and JSON data
        inline_texts = []
        for el in soup.find_all(['script', 'style']):
            if el.string:
                inline_texts.append(el.string)
        
        # Also extract from all text content for JSON data
        all_text = soup.get_text()
        inline_texts.append(all_text)
        
        blob = '\n'.join(inline_texts)
        
        # Enhanced regex to find URLs in JSON and other formats
        url_patterns = [
            r'https?://([A-Za-z0-9\-\._~%]+)(?:[:/][^\s\'"]*)?',  # Standard URLs
            r'"url":\s*"https?://([A-Za-z0-9\-\._~%]+)',  # JSON url fields
            r'"href":\s*"https?://([A-Za-z0-9\-\._~%]+)',  # JSON href fields
            r'https?://([A-Za-z0-9\-\._~%]+)\.com\.vn',  # Vietnamese domains
        ]
        
        for pattern in url_patterns:
            for m in re.finditer(pattern, blob, re.IGNORECASE):
                hosts.add(m.group(1).lower())
        
        return hosts
    
    async def detect_career_pages(self, url: str, include_subdomain_search: bool = False,
                                max_pages_to_scan: int = 100, strict_filtering: bool = False,  # Default to False
                                include_job_boards: bool = False, use_scrapy: bool = True) -> Dict:
        """
        Detect career pages with enhanced capabilities
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"ðŸ” Starting career page detection for: {url}")
            
            # Choose crawling method
            if use_scrapy:
                logger.info("ðŸ•·ï¸ Using Scrapy for career page detection")
                return await self._detect_career_pages_scrapy(url, max_pages_to_scan)
            else:
                logger.info("ðŸ”„ Using requests fallback method")
            
            # Use requests-based method
            logger.info("ðŸ”„ Using requests-based crawling method")
            
            # Step 1: Crawl the main page
            result = await crawl_single_url(url)
            
            if not result or not result.get('success'):
                return {
                    'success': False,
                    'error_message': 'Failed to crawl the website',
                    'requested_url': url,
                    'crawl_time': (datetime.now() - start_time).total_seconds()
                }
            
                        # Step 2: Extract all URLs from the page
            all_urls = result.get('urls', [])
            logger.info(f"   ðŸ“‹ Found {len(all_urls)} URLs to analyze")
            
            # Step 3: Analyze HTML content for career indicators
            html_content = result.get('html', '')
            career_indicators = self._find_career_indicators_in_html(html_content, url)
            
            # Step 4: Filter and analyze URLs
            career_pages = []
            potential_career_pages = []
            rejected_urls = []
            career_page_analysis = []
            
            # Step 5: Add career pages found from HTML analysis
            if career_indicators['career_pages']:
                career_pages.extend(career_indicators['career_pages'])
                logger.info(f"   âœ… Found career pages from HTML analysis: {career_indicators['career_pages']}")
            
            # Step 6: Analyze each URL
            for url_found in all_urls[:max_pages_to_scan]:
                analysis = await self._analyze_url_for_career(url_found, url, strict_filtering)
                
                if analysis['is_career_page']:
                    career_pages.append(url_found)
                    career_page_analysis.append(analysis)
                    logger.info(f"   âœ… Career page found: {url_found}")
                elif analysis['is_potential']:
                    potential_career_pages.append(url_found)
                    career_page_analysis.append(analysis)
                    logger.info(f"   âš ï¸ Potential career page: {url_found}")
                else:
                    rejected_urls.append({
                        'url': url_found,
                        'reason': analysis['rejection_reason']
                    })
            
            # Step 5: Subdomain search (if enabled)
            if include_subdomain_search:
                logger.info(f"   ðŸ” Starting subdomain search for: {url}")
                subdomain_results = await self._search_subdomains(url, strict_filtering)
                logger.info(f"   ðŸ“Š Subdomain search results: {len(subdomain_results['career_pages'])} career pages, {len(subdomain_results['potential_pages'])} potential pages")
                career_pages.extend(subdomain_results['career_pages'])
                potential_career_pages.extend(subdomain_results['potential_pages'])
                career_page_analysis.extend(subdomain_results['analysis'])
            else:
                # ðŸ” Enable dynamic subdomain search
                logger.info(f"   ðŸ” Starting dynamic subdomain search for: {url}")
                subdomain_results = await self._search_subdomains(url, strict_filtering)
                if subdomain_results['career_pages']:
                    logger.info(f"   âœ… Found {len(subdomain_results['career_pages'])} career pages from subdomains")
                    career_pages.extend(subdomain_results['career_pages'])
                if subdomain_results['potential_pages']:
                    logger.info(f"   âš ï¸ Found {len(subdomain_results['potential_pages'])} potential pages from subdomains")
                    potential_career_pages.extend(subdomain_results['potential_pages'])
            
            # Step 6: Job board integration (if enabled)
            if include_job_boards:
                job_board_results = await self._search_job_boards(url)
                career_pages.extend(job_board_results['career_pages'])
                career_page_analysis.extend(job_board_results['analysis'])
            
            # Step 7: Remove duplicates and validate
            career_pages = list(set(career_pages))
            potential_career_pages = list(set(potential_career_pages))
            
            # Step 8: Apply strict filtering if requested
            if strict_filtering:
                filtered_career_pages = []
                for page in career_pages:
                    # Check if this page has high confidence in analysis
                    page_analysis = next((a for a in career_page_analysis if a['url'] == page), None)
                    
                    if page_analysis and page_analysis.get('confidence', 0) >= 0.5:  # Giáº£m tá»« 0.8 xuá»‘ng 0.5
                        # Medium confidence career pages should pass validation
                        filtered_career_pages.append(page)
                        logger.info(f"âœ… Career page passed validation: {page} (score: {page_analysis['confidence']})")
                    else:
                        # Apply strict validation only for lower confidence pages
                        validation = filter_career_urls([page])
                        if validation and validation[0]['is_accepted']:
                            filtered_career_pages.append(page)
                        else:
                            potential_career_pages.append(page)
                            rejected_urls.append({
                                'url': page,
                                'reason': 'Failed strict validation'
                            })
                career_pages = filtered_career_pages
            
            # Step 9: Calculate confidence score
            confidence_score = self._calculate_confidence_score(
                len(career_pages), len(potential_career_pages), len(all_urls)
            )
            
            crawl_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'requested_url': url,
                'crawl_time': crawl_time,
                'career_pages': career_pages,
                'potential_career_pages': potential_career_pages,
                'rejected_urls': rejected_urls,
                'career_page_analysis': career_page_analysis,
                'total_urls_scanned': len(all_urls),
                'valid_career_pages': len(career_pages),
                'confidence_score': confidence_score
            }
            
        except Exception as e:
            logger.error(f"Error in career page detection: {e}")
            return {
                'success': False,
                'error_message': str(e),
                'requested_url': url,
                'crawl_time': (datetime.now() - start_time).total_seconds()
            }
    
    def _is_xml_response(self, url: str, html_content: str = None) -> bool:
        """Check if response is XML content"""
        if url.lower().endswith(('.xml', '.rss', '.atom')):
            return True
        if html_content and html_content.strip().startswith('<?xml'):
            return True
        return False
    
    async def _parse_sitemap(self, xml_text: str, base_url: str) -> List[str]:
        """Parse sitemap XML and extract job/career URLs"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(xml_text, "xml")
            
            # Extract all <loc> elements
            locs = [loc.get_text(strip=True) for loc in soup.find_all("loc")]
            
            # Filter for job/career related URLs
            job_keywords = [
                "career", "careers", "job", "jobs", "recruit", "tuyen-dung", 
                "viec-lam", "employment", "opportunity", "position", "vacancy"
            ]
            
            job_urls = []
            for url in locs:
                url_lower = url.lower()
                if any(keyword in url_lower for keyword in job_keywords):
                    job_urls.append(url)
            
            logger.info(f"   ðŸ“‹ Sitemap parsing: found {len(locs)} total URLs, {len(job_urls)} job-related")
            return job_urls
            
        except Exception as e:
            logger.warning(f"   âš ï¸ Failed to parse sitemap: {e}")
            return []

    def _is_homepage(self, url: str) -> bool:
        """Check if URL is homepage"""
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        # Check for homepage patterns
        homepage_patterns = ['/', '', '/index.html', '/index.php', '/default.html', '/default.php']
        return path in homepage_patterns and not parsed.query

    async def _analyze_url_for_career(self, url: str, base_url: str, strict_filtering: bool) -> Dict:
        """Analyze a single URL for career page indicators with improved scoring and validation"""
        analysis = {
            'url': url,
            'is_career_page': False,
            'is_potential': False,
            'confidence': 0.0,
            'rejection_reason': None,
            'indicators': []
        }
        
        try:
            # Check if this is XML/sitemap content
            if self._is_xml_response(url):
                analysis['rejection_reason'] = 'XML/sitemap content - not a career page'
                return analysis
            
            # Skip non-HTTP URLs
            if not url.startswith(('http://', 'https://')):
                analysis['rejection_reason'] = 'Non-HTTP URL'
                return analysis
            
            # EXCLUDE HOMEPAGE (HIGHEST PRIORITY)
            if self._is_homepage(url):
                analysis['rejection_reason'] = 'Homepage - not a career page'
                return analysis
            
            # Parse URL
            parsed = urlparse(url)
            path = parsed.path.lower()
            domain = parsed.netloc.lower()
            
            # IMPROVED SCORING SYSTEM (CÃ¡ch 1)
            career_indicators = []
            
            # 0. SUBDOMAIN CAREER DETECTION (HIGHEST PRIORITY)
            if domain.startswith('career.') or domain.startswith('careers.') or domain.startswith('jobs.'):
                career_indicators.append(f"Career subdomain: {domain}")
                analysis['confidence'] += 2.0  # Very high confidence for career subdomains
                analysis['is_career_page'] = True
                analysis['indicators'] = career_indicators
                return analysis
            
            # 1. Exact career keywords (HIGHEST WEIGHT)
            exact_career_keywords = ['career', 'careers', 'jobs', 'employment', 'tuyen-dung', 'viec-lam', 'co-hoi-nghe-nghiep', 'tuyen-nhan-vien']
            for keyword in exact_career_keywords:
                if keyword in path:
                    career_indicators.append(f"Exact career keyword: '{keyword}'")
                    analysis['confidence'] += 1.0  # TÄƒng tá»« 0.6 lÃªn 1.0
            
            # 2. Generic keywords (MEDIUM WEIGHT) - Chá»‰ match tá»« riÃªng biá»‡t, khÃ´ng match substring
            generic_keywords = ['dev', 'software', 'tech', 'ml', 'ai', 'testing', 'it', 'digital']
            path_segments = path.strip('/').split('/')
            for keyword in generic_keywords:
                # Chá»‰ match náº¿u keyword lÃ  má»™t segment riÃªng biá»‡t hoáº·c cÃ³ dáº¥u gáº¡ch ngang
                if (f'/{keyword}' in path or f'{keyword}/' in path or 
                    f'-{keyword}' in path or f'{keyword}-' in path or
                    keyword in path_segments):
                    career_indicators.append(f"Path contains '{keyword}'")
                    analysis['confidence'] += 0.3
            
            # 3. Career patterns (HIGH WEIGHT) - EASIER TO REACH 0.5
            career_patterns = [
                '/career', '/careers', '/jobs', '/employment', 
                '/tuyen-dung', '/viec-lam', '/co-hoi-nghe-nghiep', '/tuyen-nhan-vien',
                '/tuyendung', '/vieclam', '/cohoi', '/tuyennhanvien',
                '/recruitment', '/hiring', '/opportunities', '/positions',
                '/vacancies', '/openings', '/join-us', '/work-with-us',
                '/careers/', '/tuyen-dung/', '/viec-lam/', '/hop-tac-tuyen-dung'
            ]
            for pattern in career_patterns:
                if pattern in path:
                    career_indicators.append(f"Career pattern: {pattern}")
                    analysis['confidence'] += 1.2  # TÄƒng tá»« 0.8 lÃªn 1.2 Ä‘á»ƒ Ä‘áº£m báº£o /careers/ Ä‘Æ°á»£c nháº­n diá»‡n
            
            # 4. Domain keywords (LOW WEIGHT)
            for keyword in self.career_keywords:
                if keyword in domain:
                    career_indicators.append(f"Domain contains '{keyword}'")
                    analysis['confidence'] += 0.05  # Giáº£m tá»« 0.1 xuá»‘ng 0.05
            
            # 5. Job board domains
            for job_board in self.job_board_domains:
                if job_board in domain:
                    career_indicators.append(f"Job board domain: {job_board}")
                    analysis['confidence'] += 0.5
            
            # 6. Path depth - BONUS FOR SHALLOW PATHS
            path_depth = len([p for p in path.split('/') if p])
            if path_depth <= 2:
                career_indicators.append("Shallow path depth")
                analysis['confidence'] += 0.2  # TÄƒng tá»« 0.1 lÃªn 0.2
            elif path_depth > 4:
                analysis['rejection_reason'] = 'Path too deep'
                return analysis
            
            # IMPROVED NON-CAREER PENALTIES
            non_career_patterns = {
                '/product': -0.5,      # TÄƒng penalty
                '/service': -0.5,      # TÄƒng penalty
                '/news': -0.4,         # TÄƒng penalty
                '/blog': -0.4,         # TÄƒng penalty
                '/blogs': -0.4,        # TÄƒng penalty
                '/post': -0.4,         # TÄƒng penalty
                '/posts': -0.4,        # TÄƒng penalty
                '/article': -0.4,      # TÄƒng penalty
                '/insights': -0.4,     # TÄƒng penalty
                '/showcase': -0.4,     # TÄƒng penalty
                '/case-': -0.4,        # TÄƒng penalty
                '/about': -0.3,        # Giá»¯ nguyÃªn
                '/contact': -0.3,      # Giá»¯ nguyÃªn
                '/admin': -0.8,        # TÄƒng penalty
                '/login': -0.8,        # TÄƒng penalty
                'sitemap.xml': -1.0,   # Strong penalty for sitemap
                'robots.txt': -1.0,    # Strong penalty for robots
                '.xml': -0.8,          # Penalty for XML files
                '.json': -0.8,         # Penalty for JSON files
            }
            
            for pattern, penalty in non_career_patterns.items():
                if pattern in path:
                    analysis['confidence'] += penalty
                    if penalty <= -0.5:  # Strong penalty
                        analysis['rejection_reason'] = f'Strong non-career pattern: {pattern}'
            
            # IMPROVED VALIDATION LOGIC (CÃ¡ch 2)
            # Náº¿u confidence ráº¥t cao, luÃ´n accept (bá» qua strict validation)
            if analysis['confidence'] >= 1.0:  # Giáº£m tá»« 1.5 xuá»‘ng 1.0
                analysis['is_career_page'] = True
                analysis['is_potential'] = False
                analysis['rejection_reason'] = None  # Clear rejection reason
            elif analysis['confidence'] >= 0.8:  # Giáº£m tá»« 1.0 xuá»‘ng 0.8
                analysis['is_career_page'] = True
                analysis['is_potential'] = False
            elif analysis['confidence'] >= 0.5:  # Giáº£m tá»« 0.6 xuá»‘ng 0.5
                analysis['is_potential'] = True
            elif analysis['confidence'] < 0.0:
                # Náº¿u confidence Ã¢m, reject
                if not analysis['rejection_reason']:
                    analysis['rejection_reason'] = 'Low confidence score'
            
            analysis['indicators'] = career_indicators
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing URL {url}: {e}")
            analysis['rejection_reason'] = f'Analysis error: {str(e)}'
            return analysis
    
    async def _search_subdomains(self, base_url: str, strict_filtering: bool) -> Dict:
        """Search for career pages in subdomains"""
        subdomain_results = {
            'career_pages': [],
            'potential_pages': [],
            'analysis': []
        }
        
        try:
            parsed = urlparse(base_url)
            domain = parsed.netloc
            
            # Fallback if netloc is empty (no scheme)
            if not domain:
                domain = parsed.path.strip('/')
                logger.warning(f"   âš ï¸ No scheme in URL, using path: {domain}")
            
            # Remove www. prefix for subdomain generation
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Validate domain
            if not domain or '.' not in domain:
                logger.error(f"   âŒ Invalid domain: {domain}")
                return subdomain_results
                
            logger.info(f"   ðŸ” Parsed domain: {domain} (cleaned from {parsed.netloc})")
            
            # Dynamic subdomain discovery (NO MORE HARDCODED PATTERNS!)
            logger.info(f"   ðŸ” Starting dynamic subdomain discovery for: {domain}")
            
            # Use smart subdomain search instead of hardcoded patterns
            subdomain_urls = await self._smart_subdomain_search(base_url)
            
            logger.info(f"   ðŸ”— Dynamic discovery found {len(subdomain_urls)} subdomain URLs")
            
            logger.info(f"   ðŸ”— Generated {len(subdomain_urls)} subdomain URLs")
            logger.info(f"   ðŸ”— Sample URLs: {subdomain_urls[:3]}")
            
            # Test subdomain URLs with concurrency limit
            logger.info(f"   ðŸ” Testing {len(subdomain_urls)} subdomain URLs for {domain}")
            
            # Limit concurrency to avoid overwhelming servers
            semaphore = asyncio.Semaphore(20)  # Max 20 concurrent requests
            
            async def limited_test_subdomain(url):
                async with semaphore:
                    return await self._test_subdomain_url(url, strict_filtering)
            
            tasks = []
            for subdomain_url in subdomain_urls:
                logger.info(f"   ðŸ”— Testing subdomain: {subdomain_url}")






                task = asyncio.create_task(limited_test_subdomain(subdomain_url))
                tasks.append(task)
            
            # Test subdomain URLs with improved timeout handling
            if tasks:
                try:
                    # Use asyncio.wait with timeout to get completed tasks
                    done, pending = await asyncio.wait(
                        tasks, 
                        timeout=60,
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    
                    # Cancel pending tasks
                    for task in pending:
                        task.cancel()
                    
                    # Get results from completed tasks
                    results = []
                    for task in done:
                        try:
                            result = await task
                            results.append(result)
                        except Exception as e:
                            logger.warning(f"   âš ï¸ Task failed: {e}")
                            results.append({
                                'url': 'unknown',
                                'success': False,
                                'is_career_page': False,
                                'is_potential': False,
                                'rejection_reason': f'Task error: {str(e)}'
                            })
                    
                    logger.info(f"   âœ… Subdomain search completed: {len(results)} completed, {len(pending)} cancelled")
                    
                    # Process results with better error handling
                    for result in results:
                        if isinstance(result, dict):
                            if result.get('success'):
                                if result.get('is_career_page'):
                                    subdomain_results['career_pages'].append(result.get('url', 'unknown'))
                                elif result.get('is_potential'):
                                    subdomain_results['potential_pages'].append(result.get('url', 'unknown'))
                                subdomain_results['analysis'].append(result)
                            else:
                                logger.warning(f"   âš ï¸ Subdomain failed: {result.get('rejection_reason', 'Unknown error')}")
                        else:
                            logger.warning(f"   âš ï¸ Unexpected result type: {type(result)}")
                except Exception as e:
                    logger.error(f"   âŒ Subdomain search error: {e}")
                    results = []
            
        except Exception as e:
            logger.error(f"Error in subdomain search: {e}")
        
        return subdomain_results
    
    async def _test_subdomain_url(self, url: str, strict_filtering: bool) -> Dict:
        """Test if a subdomain URL is accessible and contains career content"""
        try:
            logger.info(f"      ðŸ”— Testing subdomain URL: {url}")
            result = await crawl_single_url(url)
            
            if result['success']:
                logger.info(f"      âœ… Subdomain crawl successful: {url}")
                analysis = await self._analyze_url_for_career(url, url, strict_filtering)
                analysis['success'] = True
                analysis['url'] = url
                return analysis
            else:
                logger.info(f"      âŒ Subdomain crawl failed: {url}")
                return {
                    'url': url,
                    'success': False,
                    'is_career_page': False,
                    'is_potential': False,
                    'rejection_reason': 'Failed to crawl'
                }
                
        except Exception as e:
            logger.error(f"      âŒ Subdomain crawl error for {url}: {e}")
            return {
                'url': url,
                'success': False,
                'is_career_page': False,
                'is_potential': False,
                'rejection_reason': f'Crawl error: {str(e)}'
            }
    
    async def _search_job_boards(self, base_url: str) -> Dict:
        """Search for company presence on job boards"""
        job_board_results = {
            'career_pages': [],
            'analysis': []
        }
        
        try:
            parsed = urlparse(base_url)
            domain = parsed.netloc
            
            # Extract company name from domain
            company_name = domain.split('.')[0]
            
            # Search on common job boards
            job_board_urls = [
                f"https://jobs.vn/company/{company_name}",
                f"https://careerlink.vn/company/{company_name}",
                f"https://topcv.vn/company/{company_name}",
                f"https://mywork.vn/company/{company_name}"
            ]
            
            # Test job board URLs
            tasks = []
            for job_board_url in job_board_urls:
                task = self._test_job_board_url(job_board_url, company_name)
                tasks.append(task)
            
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, dict) and result.get('success'):
                        if result.get('has_jobs'):
                            job_board_results['career_pages'].append(result['url'])
                        job_board_results['analysis'].append(result)
            
        except Exception as e:
            logger.error(f"Error in job board search: {e}")
        
        return job_board_results
    
    async def _test_job_board_url(self, url: str, company_name: str) -> Dict:
        """Test if a job board URL has jobs for the company"""
        try:
            result = await crawl_single_url(url)
            
            if result['success']:
                html_content = result.get('html', '').lower()
                
                # Check for job indicators
                job_indicators = [
                    'job', 'position', 'vacancy', 'opening',
                    'tuyen-dung', 'viec-lam', 'co-hoi'
                ]
                
                has_jobs = any(indicator in html_content for indicator in job_indicators)
                
                return {
                    'url': url,
                    'success': True,
                    'has_jobs': has_jobs,
                    'company_name': company_name,
                    'job_count_estimate': html_content.count('job') + html_content.count('position')
                }
            else:
                return {
                    'url': url,
                    'success': False,
                    'has_jobs': False,
                    'company_name': company_name
                }
                
        except Exception as e:
            return {
                'url': url,
                'success': False,
                'has_jobs': False,
                'company_name': company_name,
                'error': str(e)
            }
    
    async def _detect_career_pages_scrapy(self, url: str, max_pages: int) -> Dict:
        """
        Detect career pages using optimized Scrapy spider
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"ðŸš€ Running optimized Scrapy spider for: {url}")
            
            # Run Scrapy spider using new runner (increased max_pages for better coverage)
            scrapy_result = await run_spider(url, max_pages * 2)  # Double the pages for better career page detection
            
            # Always run requests-based fallback for comprehensive detection
            logger.info(f"ðŸ”„ Running requests-based fallback for comprehensive detection")
            requests_result = await self._detect_career_pages_requests_fallback(url, max_pages)
            
            # Merge results from both methods
            result = self._merge_detection_results(scrapy_result, requests_result)
            
            # Debug log
            logger.info(f"ðŸ” Scrapy result type: {type(result)}")
            logger.info(f"ðŸ” Scrapy result value: {result}")
            
            # Handle error cases first
            if not isinstance(result, dict):
                return {
                    'success': False,
                    'error_message': f'Invalid result format: {type(result)} - Expected dict, got {type(result)}',
                    'requested_url': url,
                    'crawl_time': (datetime.now() - start_time).total_seconds(),
                    'crawl_method': 'scrapy_optimized'
                }
            
            if not result.get('success', False):
                return {
                    'success': False,
                    'error_message': result.get('error_message', 'Scrapy spider failed'),
                    'requested_url': url,
                    'crawl_time': (datetime.now() - start_time).total_seconds(),
                    'crawl_method': 'scrapy_optimized'
                }
            
            # Format result to match API response
            # Handle both dict and list formats
            if isinstance(result, dict):
                career_pages_raw = result.get('career_pages', [])
                total_pages_crawled = result.get('total_pages_crawled', 0)
                career_pages_found = result.get('career_pages_found', 0)
            else:
                # If result is a list, assume it's career_pages
                career_pages_raw = result if isinstance(result, list) else []
                total_pages_crawled = 0
                career_pages_found = len(career_pages_raw)
            
            # Convert career_pages to URL list for API compatibility
            career_pages = []
            career_page_analysis = []
            all_job_urls = []  # Collect all job URLs from career pages
            
            for page_data in career_pages_raw:
                if isinstance(page_data, dict):
                    # Extract URL from dict format
                    page_url = page_data.get('url', '')
                    if page_url:
                        career_pages.append(page_url)
                        
                        # Extract job URLs from this career page
                        job_urls = page_data.get('job_urls', [])
                        if job_urls:
                            all_job_urls.extend(job_urls)
                            logger.info(f"   ðŸ”— Career page {page_url} has {len(job_urls)} job URLs")
                        
                        # Create analysis from dict data
                        analysis = {
                            'url': page_url,
                            'is_career_page': True,
                            'is_potential': False,
                            'confidence': page_data.get('confidence', 0.8),
                            'rejection_reason': None,
                            'indicators': page_data.get('indicators', ['Scrapy spider detected']),
                            'job_urls': job_urls
                        }
                        career_page_analysis.append(analysis)
                elif isinstance(page_data, str):
                    # Direct URL string
                    career_pages.append(page_data)
                    analysis = {
                        'url': page_data,
                        'is_career_page': True,
                        'is_potential': False,
                        'confidence': 0.8,
                        'rejection_reason': None,
                        'indicators': ['Scrapy spider detected'],
                        'job_urls': []
                    }
                    career_page_analysis.append(analysis)
            
            # Remove duplicate job URLs
            unique_job_urls = list(set(all_job_urls))
            logger.info(f"   ðŸ“Š Total unique job URLs found: {len(unique_job_urls)}")
            
            potential_career_pages = []
            rejected_urls = []
            
            # Add subdomain search for career pages
            logger.info(f"   ðŸ” Starting subdomain search for: {url}")
            subdomain_results = await self._search_subdomains(url, False)  # Use non-strict filtering
            logger.info(f"   ðŸ“Š Subdomain search results: {len(subdomain_results['career_pages'])} career pages, {len(subdomain_results['potential_pages'])} potential pages")
            
            # Merge subdomain results
            career_pages.extend(subdomain_results['career_pages'])
            potential_career_pages.extend(subdomain_results['potential_pages'])
            career_page_analysis.extend(subdomain_results['analysis'])
            
            # Calculate confidence score
            confidence_score = self._calculate_confidence_score(
                len(career_pages), 
                len(potential_career_pages), 
                total_pages_crawled
            )
            
            return {
                'success': True,
                'requested_url': url,
                'career_pages': career_pages,
                'potential_career_pages': potential_career_pages,
                'rejected_urls': rejected_urls,
                'career_page_analysis': career_page_analysis,
                'total_urls_scanned': total_pages_crawled,
                'valid_career_pages': len(career_pages),
                'confidence_score': confidence_score,
                'crawl_time': (datetime.now() - start_time).total_seconds(),
                'crawl_method': 'scrapy_optimized',
                'job_urls': unique_job_urls,  # Include filtered job URLs
                'total_jobs_found': len(unique_job_urls)
            }
            
        except Exception as e:
            logger.error(f"Error in Scrapy career detection: {e}")
            return {
                'success': False,
                'error_message': str(e),
                'requested_url': url,
                'crawl_time': (datetime.now() - start_time).total_seconds(),
                'crawl_method': 'scrapy_optimized'
            }

    def _find_career_indicators_in_html(self, html_content: str, base_url: str) -> Dict:
        """
        Find career indicators in HTML content using text analysis
        """
        career_indicators = {
            'career_pages': [],
            'career_texts': [],
            'confidence': 0.0
        }
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Vietnamese career keywords
            vietnamese_career_keywords = [
                'tuyá»ƒn dá»¥ng', 'tuyá»ƒn nhÃ¢n viÃªn', 'cÆ¡ há»™i nghá» nghiá»‡p', 'viá»‡c lÃ m',
                'tuyá»ƒn dá»¥ng nhÃ¢n sá»±', 'cÆ¡ há»™i viá»‡c lÃ m', 'tuyá»ƒn dá»¥ng nhÃ¢n viÃªn',
                'tuyá»ƒn dá»¥ng ká»¹ sÆ°', 'tuyá»ƒn dá»¥ng developer', 'tuyá»ƒn dá»¥ng láº­p trÃ¬nh viÃªn'
            ]
            
            # English career keywords
            english_career_keywords = [
                'career', 'careers', 'job', 'jobs', 'employment', 'hiring',
                'recruitment', 'join us', 'work with us', 'opportunities',
                'positions', 'vacancies', 'openings'
            ]
            
            all_career_keywords = vietnamese_career_keywords + english_career_keywords
            
            # Find all links with career-related text
            career_links = []
            for link in soup.find_all('a', href=True):
                link_text = link.get_text().strip().lower()
                href = link.get('href', '')
                
                # Check if link text contains career keywords
                if any(keyword in link_text for keyword in all_career_keywords):
                    career_links.append({
                        'text': link.get_text().strip(),
                        'href': href,
                        'full_url': urljoin(base_url, href)
                    })
            
            # Find career pages from links
            for link_info in career_links:
                full_url = link_info['full_url']
                if full_url.startswith(('http://', 'https://')):
                    career_indicators['career_pages'].append(full_url)
                    career_indicators['career_texts'].append(link_info['text'])
            
            # Calculate confidence based on findings
            if career_indicators['career_pages']:
                career_indicators['confidence'] = min(len(career_indicators['career_pages']) * 0.3, 1.0)
            
            logger.info(f"   ðŸ” Found {len(career_indicators['career_pages'])} career pages from HTML analysis")
            
        except Exception as e:
            logger.warning(f"   âš ï¸ Error analyzing HTML for career indicators: {e}")
        
        return career_indicators

    def _calculate_confidence_score(self, career_pages: int, potential_pages: int, total_urls: int) -> float:
        """Calculate confidence score for career page detection"""
        if total_urls == 0:
            return 0.0
        
        # Base score from career pages found
        base_score = min(career_pages * 0.3, 1.0)
        
        # Bonus for potential pages
        potential_bonus = min(potential_pages * 0.1, 0.3)
        
        # Coverage score (how many URLs were analyzed)
        coverage_score = min(total_urls / 100, 0.2)
        
        total_score = base_score + potential_bonus + coverage_score
        return min(total_score, 1.0) 

    async def _smart_subdomain_search(self, base_url: str) -> List[str]:
        """
        Smart subdomain search with 3-tier approach:
        1. Dynamic discovery from base domain content
        2. DNS/CT enumeration (if available)
        3. Minimal fallback from config (NO hardcoding)
        """
        root_domain, netloc = self._safe_domain(base_url)
        subdomains: List[str] = []
        
        logger.info(f"   ðŸ” Smart subdomain search for root domain: {root_domain}")
        
        # 1) Dynamic discovery (primary)
        discovered = await self._discover_subdomains_dynamically(base_url)
        subdomains.extend(discovered)
        logger.info(f"   âœ… Dynamic discovery found: {len(discovered)} subdomains")
        
        # 2) DNS/CT enumeration (secondary) - placeholder for future
        # dns_found = await self._enumerate_subdomains_dns(root_domain)
        # subdomains.extend(dns_found)
        
        # Dedup early
        subdomains = sorted(set(subdomains))
        
        # 3) Always add essential career subdomains (ALWAYS TRY THESE)
        fallback = self._get_minimal_fallback_patterns(root_domain)
        subdomains.extend(fallback)
        logger.info(f"   ðŸ”§ Always trying essential career subdomains: {len(fallback)} patterns")
        
        # Final dedup
        final_subdomains = sorted(set(subdomains))
        logger.info(f"   ðŸŽ¯ Total unique subdomains found: {len(final_subdomains)}")
        
        return final_subdomains
    
    async def _discover_subdomains_dynamically(self, base_url: str) -> List[str]:
        """
        Discover subdomains from base domain HTML content
        """
        root_domain, netloc = self._safe_domain(base_url)
        start_url = base_url if '://' in base_url else f"https://{netloc}"
        
        candidates: Set[str] = set()
        alive_urls: List[str] = []
        
        try:
            # Fetch base HTML
            result = await crawl_single_url(start_url)
            if not result or not result.get('success') or not result.get('html'):
                logger.warning(f"   âš ï¸ Failed to fetch base domain HTML")
                return []
            
            html = result.get('html', '')
            logger.info(f"   ðŸ“„ Base domain HTML length: {len(html)}")
            
            # Collect hosts from HTML
            hosts = self._collect_hosts_from_html(html, start_url)
            logger.info(f"   ðŸ” Found {len(hosts)} total hosts in HTML")
            
            # Filter: only keep subdomains of root_domain
            for h in hosts:
                if self._is_subdomain_of(h, root_domain):
                    candidates.add(h)
            
            logger.info(f"   ðŸŽ¯ Filtered to {len(candidates)} candidate subdomains")
            
            if not candidates:
                return []
            
            # Validate subdomains with concurrency limit
            sem = asyncio.Semaphore(self.MAX_CONCURRENCY)
            
            async def limited_validate(host: str):
                async with sem:
                    return await self._validate_host_alive(host)
            
            # Create tasks explicitly to avoid coroutine error
            tasks = [asyncio.create_task(limited_validate(h)) for h in candidates]
            
            # Use asyncio.wait with timeout
            done, pending = await asyncio.wait(tasks, timeout=self.GLOBAL_TIMEOUT)
            
            # Cancel pending tasks
            for task in pending:
                task.cancel()
            
            # Collect results
            for task in done:
                try:
                    result = task.result()
                    if result and result.get('alive'):
                        alive_urls.append(result.get('url', ''))
                except Exception as e:
                    logger.warning(f"   âš ï¸ Task failed: {e}")
                    continue
            
            logger.info(f"   âœ… Validation completed: {len(alive_urls)} alive subdomains")
            
        except Exception as e:
            logger.error(f"   âŒ Dynamic discovery error: {e}")
            # Don't re-raise the exception, just return empty list
            # This prevents the entire career page detection from failing
        
        return alive_urls
    
    async def _validate_host_alive(self, host: str) -> Dict:
        """
        Validate if host is alive (DNS + HTTP) with timeout
        """
        try:
            # Add timeout wrapper to prevent hanging
            async def validate_with_timeout():
                # Simple validation using existing crawl_single_url
                url = f"https://{host}"
                result = await crawl_single_url(url)
                
                if result and result.get('success'):
                    return {
                        'alive': True,
                        'url': url,
                        'host': host
                    }
                else:
                    # Try HTTP as fallback
                    url = f"http://{host}"
                    result = await crawl_single_url(url)
                    
                    if result and result.get('success'):
                        return {
                            'alive': True,
                            'url': url,
                            'host': host
                        }
                
                return {
                    'alive': False,
                    'url': url,
                    'host': host
                }
            
            # Use asyncio.wait_for with timeout
            result = await asyncio.wait_for(validate_with_timeout(), timeout=10.0)
            return result
            
        except asyncio.TimeoutError:
            logger.warning(f"      âš ï¸ Host validation timeout for {host}")
            return {
                'alive': False,
                'url': f"https://{host}",
                'host': host,
                'error': 'timeout'
            }
        except Exception as e:
            logger.warning(f"      âš ï¸ Host validation failed for {host}: {e}")
            return {
                'alive': False,
                'url': f"https://{host}",
                'host': host,
                'error': str(e)
            }
    
    def _get_minimal_fallback_patterns(self, domain: str) -> List[str]:
        """
        Minimal fallback patterns including essential career subdomains
        """
        # Essential career subdomains (always include)
        career_subdomains = ['career', 'careers', 'jobs', 'tuyen-dung', 'viec-lam']
        
        # Read from environment variable
        raw = os.getenv("CRAWLER_FALLBACK_SUBDOMAINS", "").strip()
        if raw:
            # Parse comma-separated patterns
            env_tags = [t.strip().lower() for t in raw.split(',') if t.strip()]
            career_subdomains.extend(env_tags)
        
        # Create URLs
        urls = [f"https://{t}.{domain}" for t in career_subdomains]
        
        logger.info(f"   ðŸ”§ Using fallback patterns: {career_subdomains}")
        return urls
    
    async def _detect_career_pages_requests_fallback(self, url: str, max_pages: int) -> Dict:
        """
        Fallback career page detection using requests when Scrapy fails
        """
        try:
            logger.info(f"ðŸ”„ Using requests-based fallback for: {url}")
            
            # Use existing requests-based logic
            from .crawler import crawl_single_url
            result = await crawl_single_url(url)
            
            if not result['success']:
                return {
                    'success': False,
                    'error_message': 'Failed to crawl with requests fallback',
                    'requested_url': url,
                    'crawl_time': 0,
                    'crawl_method': 'requests_fallback'
                }
            
            # Extract links from HTML
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(result['html'], 'html.parser')
            
            # Find career-related links
            career_links = []
            all_links = soup.find_all('a', href=True)
            
            career_keywords = ['career', 'careers', 'job', 'jobs', 'tuyen-dung', 'viec-lam']
            
            for link in all_links:
                href = link.get('href', '').lower()
                text = link.get_text().lower()
                
                if any(keyword in href or keyword in text for keyword in career_keywords):
                    from urllib.parse import urljoin
                    full_url = urljoin(result['url'], href)
                    career_links.append(full_url)
            
            # Return result in same format as Scrapy
            return {
                'success': True,
                'requested_url': url,
                'career_pages': career_links,
                'total_pages_crawled': 1,
                'career_pages_found': len(career_links),
                'crawl_time': 0,
                'crawl_method': 'requests_fallback',
                'contact_info': {
                    'emails': [],
                    'phones': [],
                    'contact_urls': []
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Error in requests fallback: {e}")
            return {
                'success': False,
                'error_message': str(e),
                'requested_url': url,
                'crawl_time': 0,
                'crawl_method': 'requests_fallback'
            }
    
    def _merge_detection_results(self, scrapy_result: Dict, requests_result: Dict) -> Dict:
        """
        Merge results from Scrapy and requests-based detection
        """
        try:
            # Extract career pages from both results
            scrapy_pages = scrapy_result.get('career_pages', [])
            requests_pages = requests_result.get('career_pages', [])
            
            # ðŸŽ¯ PRIORITIZE SCRAPY RESULTS: Only use requests results if Scrapy found nothing
            if scrapy_pages:
                # Scrapy found career pages, use only Scrapy results
                unique_pages = scrapy_pages
                logger.info(f"ðŸ” Using Scrapy results only: {len(scrapy_pages)} pages")
            else:
                # Scrapy found nothing, use requests results but filter out homepages
                filtered_requests_pages = []
                for page in requests_pages:
                    if not self._is_homepage(page):
                        filtered_requests_pages.append(page)
                    else:
                        logger.info(f"ðŸš« Filtered out homepage from requests: {page}")
                
                unique_pages = filtered_requests_pages
                logger.info(f"ðŸ” Using filtered requests results: {len(filtered_requests_pages)} pages (filtered from {len(requests_pages)})")
            
            # ðŸš« REMOVE DUPLICATES: Ensure no duplicate URLs
            seen_urls = set()
            deduplicated_pages = []
            for page in unique_pages:
                if isinstance(page, dict):
                    url = page.get('url', '')
                else:
                    url = str(page)
                
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    deduplicated_pages.append(page)
                else:
                    logger.info(f"ðŸš« Removed duplicate: {url}")
            
            unique_pages = deduplicated_pages
            logger.info(f"ðŸ” After deduplication: {len(unique_pages)} unique pages")
            
            # ðŸŽ¯ LIMIT TO TOP 3 CAREER PAGES: Prioritize main career pages
            if len(unique_pages) > 3:
                # Sort by priority: main career pages first, then job listings
                def career_page_priority(url):
                    if '/tuyen-dung.html' in url and '/item/' not in url:
                        return 1  # Main career page - highest priority
                    elif '/tuyen-dung/' in url:
                        return 2  # Job listings - medium priority
                    elif '/careers' in url or '/jobs' in url:
                        return 3  # English career pages - lower priority
                    else:
                        return 4  # Other pages - lowest priority
                
                # Sort by priority, then take top 3
                sorted_pages = sorted(unique_pages, key=career_page_priority)
                unique_pages = sorted_pages[:3]
                logger.info(f"ðŸŽ¯ Limited to top 3 career pages: {unique_pages}")
            else:
                logger.info(f"ðŸŽ¯ Using all {len(unique_pages)} career pages (â‰¤3)")
            
            # Merge contact info
            scrapy_contact = scrapy_result.get('contact_info', {})
            requests_contact = requests_result.get('contact_info', {})
            
            merged_contact = {
                'emails': list(set(scrapy_contact.get('emails', []) + requests_contact.get('emails', []))),
                'phones': list(set(scrapy_contact.get('phones', []) + requests_contact.get('phones', []))),
                'contact_urls': list(set(scrapy_contact.get('contact_urls', []) + requests_contact.get('contact_urls', [])))
            }
            
            # Calculate combined stats
            total_pages_crawled = scrapy_result.get('total_pages_crawled', 0) + requests_result.get('total_pages_crawled', 0)
            total_crawl_time = scrapy_result.get('crawl_time', 0) + requests_result.get('crawl_time', 0)
            
            # Determine primary method
            primary_method = 'scrapy_optimized' if scrapy_result.get('total_pages_crawled', 0) > 0 else 'requests_fallback'
            
            logger.info(f"ðŸ”— Merged results: {len(scrapy_pages)} Scrapy + {len(requests_pages)} requests = {len(unique_pages)} unique pages")
            
            return {
                'success': True,
                'requested_url': scrapy_result.get('requested_url', requests_result.get('requested_url', '')),
                'career_pages': unique_pages,
                'total_pages_crawled': total_pages_crawled,
                'career_pages_found': len(unique_pages),
                'crawl_time': total_crawl_time,
                'crawl_method': f'{primary_method}_merged',
                'contact_info': merged_contact,
                'detection_methods': {
                    'scrapy': {
                        'pages_found': len(scrapy_pages),
                        'pages_crawled': scrapy_result.get('total_pages_crawled', 0),
                        'success': scrapy_result.get('success', False)
                    },
                    'requests': {
                        'pages_found': len(requests_pages),
                        'pages_crawled': requests_result.get('total_pages_crawled', 0),
                        'success': requests_result.get('success', False)
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ Error merging detection results: {e}")
            # Fallback to requests result if merging fails
            return requests_result 